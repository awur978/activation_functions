# activation_functions
step, sigmoid, relu, softmax, softplus, tanh, prelu, elu, swish activation functions for deep neural networks
